import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
import os
import requests
import time
import warnings

# ë¶ˆí•„ìš”í•œ FutureWarning ìˆ¨ê¸°ê¸°
warnings.simplefilter(action='ignore', category=FutureWarning)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data')

# Bitget ì„ ë¬¼ ì½”ì¸ ëª©ë¡
BITGET_SYMBOLS = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT']

# ì—…ë¹„íŠ¸ ì½”ì¸ ëª©ë¡
UPBIT_SYMBOLS = [
    'KRW-ADA', 'KRW-ANKR', 'KRW-AVAX', 'KRW-AXS', 'KRW-BCH',
    'KRW-BTC', 'KRW-CRO', 'KRW-DOGE', 'KRW-ETH', 'KRW-HBAR',
    'KRW-IMX', 'KRW-MANA', 'KRW-MVL', 'KRW-SAND', 'KRW-SOL',
    'KRW-THETA', 'KRW-VET', 'KRW-WAXP', 'KRW-XLM', 'KRW-XRP'
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_last_completed_candle_time(interval: str) -> datetime:
    """ì™„ë£Œëœ ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ ê³„ì‚° (UTC ê¸°ì¤€)"""
    now = datetime.now(timezone.utc)
    
    if interval == '1d':
        # ì¼ë´‰: ì–´ì œê¹Œì§€ ì™„ë£Œ
        last_complete = now.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)
    elif interval == '4h':
        # 4ì‹œê°„ë´‰: í˜„ì¬ ì‹œê°„ ê¸°ì¤€ ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤
        # ìº”ë“¤ ì‹œì‘ ì‹œê°„: 0, 4, 8, 12, 16, 20ì‹œ
        current_hour = now.hour
        last_candle_hour = (current_hour // 4) * 4
        last_complete = now.replace(hour=last_candle_hour, minute=0, second=0, microsecond=0) - timedelta(hours=4)
    else:
        last_complete = now - timedelta(hours=1)
    
    return last_complete


def load_existing_csv(filepath: str) -> pd.DataFrame:
    """ê¸°ì¡´ CSV íŒŒì¼ ë¡œë“œ"""
    if not os.path.exists(filepath):
        return None
    
    try:
        df = pd.read_csv(filepath)
        
        # datetime ì»¬ëŸ¼ ì²˜ë¦¬
        if 'date' in df.columns:
            df['datetime'] = pd.to_datetime(df['date'])
        elif 'datetime' in df.columns:
            df['datetime'] = pd.to_datetime(df['datetime'])
        
        df.set_index('datetime', inplace=True)
        # ì¤‘ìš”: CSV ë°ì´í„°ëŠ” Timezone ì •ë³´ ì—†ì´ ë¡œë“œ (Naive)
        df.index = df.index.tz_localize(None)
        
        return df
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return None


def save_csv(df: pd.DataFrame, filepath: str, date_col: str = 'datetime'):
    """CSV íŒŒì¼ ì €ì¥"""
    df_save = df.copy()
    df_save = df_save.reset_index()
    
    # ì»¬ëŸ¼ëª… ì •ë¦¬
    if 'index' in df_save.columns:
        df_save = df_save.rename(columns={'index': date_col})
    
    # ì¤‘ë³µ ì œê±° (datetime ê¸°ì¤€)
    df_save = df_save.drop_duplicates(subset=[date_col], keep='last')
    df_save = df_save.sort_values(date_col)
    
    df_save.to_csv(filepath, index=False)
    print(f"âœ… Saved {len(df_save)} rows to {filepath}")


def merge_and_dedupe(existing: pd.DataFrame, new: pd.DataFrame) -> pd.DataFrame:
    """ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•© ë° ì¤‘ë³µ ì œê±°"""
    if existing is None or len(existing) == 0:
        return new
    if new is None or len(new) == 0:
        return existing
    
    combined = pd.concat([existing, new])
    combined = combined[~combined.index.duplicated(keep='last')]
    combined = combined.sort_index()
    
    return combined

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TQQQ ë°ì´í„° ì—…ë°ì´íŠ¸ (yfinance)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def update_tqqq():
    """TQQQ ì¼ë´‰ ë°ì´í„° ì—…ë°ì´íŠ¸"""
    print("\nğŸ“ˆ Updating TQQQ daily data...")
    
    filepath = os.path.join(DATA_DIR, 'tqqq_daily.csv')
    existing = load_existing_csv(filepath)
    
    # ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤ ì‹œê°„ (UTC)
    last_complete = get_last_completed_candle_time('1d')
    # ë¹„êµë¥¼ ìœ„í•´ Timezone ì •ë³´ ì œê±° (Naive)
    last_complete = last_complete.replace(tzinfo=None)
    
    # ì‹œì‘ ë‚ ì§œ ê²°ì •
    if existing is not None and len(existing) > 0:
        last_date = existing.index.max()
        start_date = last_date + timedelta(days=1)
        
        if start_date.date() > last_complete.date():
            print(f"  â„¹ï¸ Already up to date (last: {last_date.date()})")
            return
    else:
        # ìƒˆë¡œ ì‹œì‘: 3ë…„ ì „ë¶€í„°
        start_date = last_complete - timedelta(days=365*3)
    
    try:
        import yfinance as yf
        
        end_date = last_complete + timedelta(days=1)
        # auto_adjust=False ì¶”ê°€í•˜ì—¬ ê²½ê³  í•´ê²° ë° ë°ì´í„° ì¼ê´€ì„± í™•ë³´
        data = yf.download('TQQQ', start=start_date, end=end_date, progress=False, auto_adjust=False)
        
        if data.empty:
            print("  âš ï¸ No new data available")
            return
        
        # ì»¬ëŸ¼ ì •ë¦¬
        if isinstance(data.columns, pd.MultiIndex):
            data.columns = data.columns.get_level_values(0)
        
        data.columns = [c.lower() for c in data.columns]
        data.index = data.index.tz_localize(None)
        data.index.name = 'datetime'
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ
        data = data[['open', 'high', 'low', 'close', 'volume']]
        
        # ë³‘í•©
        combined = merge_and_dedupe(existing, data)
        
        # ì €ì¥
        save_csv(combined, filepath, date_col='date')
        print(f"  ğŸ“Š TQQQ: {len(data)} new rows added")
        
    except Exception as e:
        print(f"  âŒ Error updating TQQQ: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Bitget (Binance Futures) ë°ì´í„° ì—…ë°ì´íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_binance_futures(symbol: str, interval: str, start_time: datetime, end_time: datetime) -> pd.DataFrame:
    """Binance Futures APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    url = "https://fapi.binance.com/fapi/v1/klines"
    
    all_data = []
    # Timezone ì •ë³´ê°€ ìˆë‹¤ë©´ timestampë¡œ ë³€í™˜ ì‹œ ê³ ë ¤ë¨
    current_start = int(start_time.replace(tzinfo=timezone.utc).timestamp() * 1000)
    end_ts = int(end_time.replace(tzinfo=timezone.utc).timestamp() * 1000)
    
    while current_start < end_ts:
        params = {
            'symbol': symbol,
            'interval': interval,
            'startTime': current_start,
            'endTime': end_ts,
            'limit': 1000
        }
        
        response = requests.get(url, params=params, timeout=15)
        if response.status_code != 200:
            break
        
        data = response.json()
        if not data:
            break
        
        all_data.extend(data)
        current_start = data[-1][0] + 1
        
        time.sleep(0.1)  # Rate limit
    
    if not all_data:
        return None
    
    df = pd.DataFrame(all_data, columns=[
        'timestamp', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_volume', 'trades', 'taker_buy_base',
        'taker_buy_quote', 'ignore'
    ])
    
    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
    df.set_index('datetime', inplace=True)
    df.index = df.index.tz_localize(None) # Naiveë¡œ ë³€í™˜
    
    for col in ['open', 'high', 'low', 'close', 'volume']:
        df[col] = df[col].astype(float)
    
    return df[['open', 'high', 'low', 'close', 'volume']]


def update_bitget():
    """Bitget (Binance Futures) 4H ë°ì´í„° ì—…ë°ì´íŠ¸"""
    print("\nğŸ”¶ Updating Bitget (Binance Futures) 4H data...")
    
    last_complete = get_last_completed_candle_time('4h')
    # [ìˆ˜ì •] ë¹„êµ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ Timezone ì œê±° (Naiveë¡œ í†µì¼)
    last_complete = last_complete.replace(tzinfo=None)
    
    for symbol in BITGET_SYMBOLS:
        name = symbol.replace('USDT', '').lower()
        filepath = os.path.join(DATA_DIR, f'bitget_{name}_4h.csv')
        
        existing = load_existing_csv(filepath)
        
        # ì‹œì‘ ì‹œê°„ ê²°ì •
        if existing is not None and len(existing) > 0:
            last_date = existing.index.max()
            start_time = last_date + timedelta(hours=4)
            
            # ì—¬ê¸°ì„œ offset-naive vs offset-aware ì—ëŸ¬ê°€ ë°œìƒí–ˆì—ˆìŒ -> ì´ì œ ë‘˜ ë‹¤ Naiveë¼ í•´ê²°ë¨
            if start_time > last_complete:
                print(f"  â„¹ï¸ {symbol}: Already up to date")
                continue
        else:
            # ìƒˆë¡œ ì‹œì‘: 3ë…„ ì „ë¶€í„°
            start_time = last_complete - timedelta(days=365*3)
        
        try:
            new_data = fetch_binance_futures(symbol, '4h', start_time, last_complete + timedelta(hours=4))
            
            if new_data is None or len(new_data) == 0:
                print(f"  âš ï¸ {symbol}: No new data")
                continue
            
            # ì™„ë£Œëœ ìº”ë“¤ë§Œ í•„í„°ë§
            new_data = new_data[new_data.index <= last_complete]
            
            if len(new_data) == 0:
                print(f"  â„¹ï¸ {symbol}: No completed candles yet")
                continue
            
            # ë³‘í•©
            combined = merge_and_dedupe(existing, new_data)
            
            # ì €ì¥
            save_csv(combined, filepath)
            print(f"  ğŸ“Š {symbol}: {len(new_data)} new rows added")
            
        except Exception as e:
            print(f"  âŒ Error updating {symbol}: {e}")
        
        time.sleep(0.2)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì—…ë¹„íŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_upbit_ohlcv(market: str, interval: str, count: int = 200, to: str = None) -> pd.DataFrame:
    """ì—…ë¹„íŠ¸ APIì—ì„œ OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if interval == '4h':
        url = "https://api.upbit.com/v1/candles/minutes/240"
    elif interval == '1d':
        url = "https://api.upbit.com/v1/candles/days"
    else:
        return None
    
    params = {'market': market, 'count': count}
    if to:
        params['to'] = to
    
    headers = {"accept": "application/json"}
    response = requests.get(url, params=params, headers=headers, timeout=15)
    
    if response.status_code != 200:
        return None
    
    data = response.json()
    if not data:
        return None
    
    df = pd.DataFrame(data)
    
    # ì»¬ëŸ¼ ë§¤í•‘
    df = df.rename(columns={
        'candle_date_time_kst': 'datetime',
        'opening_price': 'open',
        'high_price': 'high',
        'low_price': 'low',
        'trade_price': 'close',
        'candle_acc_trade_volume': 'volume'
    })
    
    df['datetime'] = pd.to_datetime(df['datetime'])
    df.set_index('datetime', inplace=True)
    df = df.sort_index()
    
    return df[['open', 'high', 'low', 'close', 'volume']]


def fetch_upbit_full(market: str, interval: str, start_time: datetime, end_time: datetime) -> pd.DataFrame:
    """ì—…ë¹„íŠ¸ì—ì„œ ì „ì²´ ê¸°ê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§•)"""
    all_data = []
    to_time = end_time.strftime('%Y-%m-%dT%H:%M:%S')
    
    max_iterations = 50  # ìµœëŒ€ 50ë²ˆ í˜¸ì¶œ (ì•½ 10000ê°œ ë°ì´í„°)
    
    for _ in range(max_iterations):
        df = fetch_upbit_ohlcv(market, interval, count=200, to=to_time)
        
        if df is None or len(df) == 0:
            break
        
        # ì‹œì‘ ì‹œê°„ ì´ì „ ë°ì´í„° ì œì™¸
        df = df[df.index >= start_time]
        
        if len(df) == 0:
            break
        
        all_data.append(df)
        
        # ë‹¤ìŒ í˜ì´ì§€
        oldest = df.index.min()
        if oldest <= start_time:
            break
        
        to_time = (oldest - timedelta(seconds=1)).strftime('%Y-%m-%dT%H:%M:%S')
        time.sleep(0.1)
    
    if not all_data:
        return None
    
    combined = pd.concat(all_data)
    combined = combined[~combined.index.duplicated(keep='first')]
    combined = combined.sort_index()
    
    return combined


def update_upbit():
    """ì—…ë¹„íŠ¸ 4H/1D ë°ì´í„° ì—…ë°ì´íŠ¸"""
    print("\nğŸŸ  Updating Upbit data...")
    
    last_complete_4h = get_last_completed_candle_time('4h')
    last_complete_1d = get_last_completed_candle_time('1d')
    
    # [ìˆ˜ì •] Timezone ì œê±° (Naiveë¡œ í†µì¼í•˜ì—¬ ê³„ì‚°)
    last_complete_4h = last_complete_4h.replace(tzinfo=None)
    last_complete_1d = last_complete_1d.replace(tzinfo=None)
    
    # í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (ê°’ë§Œ +9ì‹œê°„, Naive ìœ ì§€)
    kst_offset = timedelta(hours=9)
    last_complete_4h_kst = last_complete_4h + kst_offset
    last_complete_1d_kst = last_complete_1d + kst_offset
    
    for market in UPBIT_SYMBOLS:
        symbol = market.replace('KRW-', '').lower()
        
        # 4H ë°ì´í„°
        filepath_4h = os.path.join(DATA_DIR, f'upbit_{symbol}_4h.csv')
        existing_4h = load_existing_csv(filepath_4h)
        
        if existing_4h is not None and len(existing_4h) > 0:
            last_date = existing_4h.index.max()
            start_time = last_date + timedelta(hours=4)
        else:
            start_time = last_complete_4h_kst - timedelta(days=365*3)
        
        if start_time <= last_complete_4h_kst:
            try:
                new_data = fetch_upbit_full(market, '4h', start_time, last_complete_4h_kst + timedelta(hours=4))
                
                if new_data is not None and len(new_data) > 0:
                    # ì™„ë£Œëœ ìº”ë“¤ë§Œ
                    new_data = new_data[new_data.index <= last_complete_4h_kst]
                    
                    if len(new_data) > 0:
                        combined = merge_and_dedupe(existing_4h, new_data)
                        save_csv(combined, filepath_4h)
                        print(f"  ğŸ“Š {market} 4H: {len(new_data)} new rows")
            except Exception as e:
                print(f"  âŒ Error {market} 4H: {e}")
        
        # 1D ë°ì´í„°
        filepath_1d = os.path.join(DATA_DIR, f'upbit_{symbol}_1d.csv')
        existing_1d = load_existing_csv(filepath_1d)
        
        if existing_1d is not None and len(existing_1d) > 0:
            last_date = existing_1d.index.max()
            start_time = last_date + timedelta(days=1)
        else:
            start_time = last_complete_1d_kst - timedelta(days=365*3)
        
        if start_time <= last_complete_1d_kst:
            try:
                new_data = fetch_upbit_full(market, '1d', start_time, last_complete_1d_kst + timedelta(days=1))
                
                if new_data is not None and len(new_data) > 0:
                    # ì™„ë£Œëœ ìº”ë“¤ë§Œ
                    new_data = new_data[new_data.index <= last_complete_1d_kst]
                    
                    if len(new_data) > 0:
                        combined = merge_and_dedupe(existing_1d, new_data)
                        save_csv(combined, filepath_1d)
                        print(f"  ğŸ“Š {market} 1D: {len(new_data)} new rows")
            except Exception as e:
                print(f"  âŒ Error {market} 1D: {e}")
        
        time.sleep(0.2)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("=" * 60)
    print("ğŸ“Š Trading Data Auto-Update")
    print(f"â° Current time (UTC): {datetime.now(timezone.utc)}")
    print("=" * 60)
    
    # ë°ì´í„° í´ë” ìƒì„±
    os.makedirs(DATA_DIR, exist_ok=True)
    
    # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì–´ë–¤ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì •
    now = datetime.now(timezone.utc)
    hour = now.hour
    
    # TQQQ: UTC 21ì‹œ (í•œêµ­ì‹œê°„ í™”~í†  06ì‹œ) ì „í›„ì— ì‹¤í–‰
    if 20 <= hour <= 22 or hour <= 1:
        update_tqqq()
    
    # 4H ë°ì´í„°: í•­ìƒ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ì—ì„œ ì‹œê°„ ê´€ë¦¬)
    update_bitget()
    update_upbit()
    
    print("\n" + "=" * 60)
    print("âœ… Update completed!")
    print("=" * 60)


if __name__ == "__main__":
    main()
